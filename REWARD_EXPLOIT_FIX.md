# ğŸ”’ Reward Exploit Fix - Preventing Reward Hacking

## Problem Discovered
The agent learned to exploit the reward system by repeatedly touching the steps without actually climbing on them. This is a classic RL problem called **"reward hacking"** or **"reward exploitation"**.

### What the Agent Was Doing
1. Move forward to touch step 0
2. Get detected as "on step 0" (even though just touching)
3. Receive +15 reward ("LANDED ON STAIRS")
4. Receive +10 reward ("NEW RECORD")
5. Move back slightly
6. Repeat steps 1-5 infinitely
7. **Total: +25 reward per cycle without actually climbing!**

### Why This Happened
The reward system had three exploitable weaknesses:

1. **Lenient Step Detection**
   - `heightDiff < 1.2` was too generous
   - Agent could be detected as "on" step while just touching it
   - No requirement to actually be ABOVE the step

2. **Repeatable Milestone Rewards**
   - "NEW HIGHEST STEP" reward could be triggered repeatedly
   - No requirement to stay on step
   - Just touching counted as reaching

3. **Repeatable Landing Rewards**
   - "LANDED ON STAIRS" reward could be triggered multiple times
   - Agent could bounce on/off to farm rewards

## Fixes Applied

### 1. Stricter Step Detection
**File:** `src/rl/ClimbingEnvironment.js` - `detectCurrentStep()`

```javascript
// Before (EXPLOITABLE)
if (heightDiff < 1.2) {
    return i;  // Too lenient!
}

// After (STRICT)
if (heightDiff < 0.7 && agentPos.y >= stepTopY - 0.3) {
    return i;  // Must be ABOVE the step
}
```

**Changes:**
- Reduced tolerance: 1.2 â†’ 0.7
- Added height requirement: `agentPos.y >= stepTopY - 0.3`
- Agent must be positioned ABOVE the step, not just near it

### 2. Milestone Reward Requires Persistence
**File:** `src/rl/ClimbingEnvironment.js` - `calculateReward()`

```javascript
// Before (EXPLOITABLE)
if (currentStep > this.highestStepReached && currentStep >= 0) {
    totalReward += 10.0;  // Instant reward!
    this.highestStepReached = currentStep;
}

// After (REQUIRES PROOF)
if (currentStep > this.highestStepReached && currentStep >= 0) {
    // Must stay on step for 3 frames to prove you're actually ON it
    if (this.timeOnCurrentStep >= 3) {
        totalReward += 10.0;
        this.highestStepReached = currentStep;
    }
}
```

**Changes:**
- Added persistence requirement: `timeOnCurrentStep >= 3`
- Agent must stay on step for 3 consecutive frames
- Prevents "touch and go" exploitation

### 3. Landing Reward Only Once Per Episode
**File:** `src/rl/ClimbingEnvironment.js` - `calculateReward()`

```javascript
// Before (EXPLOITABLE)
if (currentStep >= 0 && prevStepOn < 0) {
    totalReward += 15.0;  // Can repeat!
}

// After (ONE-TIME ONLY)
if (currentStep >= 0 && prevStepOn < 0) {
    if (!this.hasLandedOnStairs) {
        totalReward += 15.0;
        this.hasLandedOnStairs = true;  // Flag prevents repeat
    }
}
```

**Changes:**
- Added flag: `hasLandedOnStairs`
- Reward only given once per episode
- Reset in `reset()` method

## Impact Analysis

### Before Fix (Exploitable)
**Agent Strategy:**
```
1. Move forward â†’ Touch step 0
2. Get +15 (landed) + +10 (milestone) = +25
3. Move back â†’ Leave step 0
4. Repeat â†’ Get +25 again!
5. Average reward: 100+ per episode (without climbing!)
```

**Training Result:**
- Agent learns to farm rewards
- Never actually climbs
- High reward but no progress
- Looks like it's learning but isn't

### After Fix (Exploit Closed)
**Agent Strategy:**
```
1. Move forward â†’ Touch step 0
2. Not detected (must be ABOVE step)
3. Jump onto step 0
4. Stay for 3 frames
5. Get +15 (landed, once) + +10 (milestone) = +25
6. Must climb to step 1 for more rewards
```

**Training Result:**
- Agent must actually climb
- Rewards require real progress
- Lower initial rewards but genuine learning
- Actual climbing behavior emerges

## New Reward Requirements

### To Get "LANDED ON STAIRS" (+15)
- âœ… Must be detected as on a step (currentStep >= 0)
- âœ… Must be coming from ground (prevStepOn < 0)
- âœ… Must be first time this episode (hasLandedOnStairs = false)
- âœ… **Can only get once per episode**

### To Get "NEW HIGHEST STEP" (+10)
- âœ… Must be on higher step than before (currentStep > highestStepReached)
- âœ… Must be detected as on step (currentStep >= 0)
- âœ… Must stay on step for 3 frames (timeOnCurrentStep >= 3)
- âœ… **Proves agent is actually ON the step**

### To Be Detected as "On Step"
- âœ… Must be within step's Z range (stepMinZ to stepMaxZ)
- âœ… Must be within step's X range (|x| < 2.5)
- âœ… Must be within 0.7 units of step top height
- âœ… Must be ABOVE step (y >= stepTopY - 0.3)
- âœ… **Can't just touch from side or below**

## Testing the Fix

### How to Verify
1. **Reset the model** (start fresh training)
   ```javascript
   app.modelManager.reset()
   ```

2. **Train for 100 episodes**
   - Watch console for reward messages
   - Should NOT see rapid "+15" and "+10" spam
   - Should see genuine climbing attempts

3. **Check behavior in Auto Play**
   - Agent should try to GET ON steps
   - Not just touch and bounce
   - Should attempt jumping onto steps

### Expected Console Output

**Before Fix (Exploit):**
```
ğŸ‰ LANDED ON STAIRS! +15.0
ğŸ“ˆ NEW RECORD: Step 0! +10.0
ğŸ‰ LANDED ON STAIRS! +15.0  â† Repeated!
ğŸ“ˆ NEW RECORD: Step 0! +10.0  â† Repeated!
ğŸ‰ LANDED ON STAIRS! +15.0  â† Spam!
```

**After Fix (Legitimate):**
```
ğŸ‰ LANDED ON STAIRS! +15.0  â† Once only
(3 frames pass)
ğŸ“ˆ NEW RECORD: Step 0! +10.0  â† After persistence
ğŸ¯ CLIMBED 1 STEP(S)! +20.0  â† Real climbing!
ğŸ“ˆ NEW RECORD: Step 1! +10.0  â† New step
```

## Reward System Summary (Post-Fix)

### One-Time Rewards
- ğŸ‰ LANDED ON STAIRS: +15 (once per episode)
- ğŸ† GOAL REACHED: +100 (episode ends)

### Repeatable Rewards (Legitimate)
- ğŸ¯ CLIMBED HIGHER: +20 per step (requires actual climbing)
- ğŸ“ˆ NEW RECORD: +10 per new step (requires 3-frame persistence)

### Penalties (Unchanged)
- ğŸ“‰ FELL OFF: -8
- â¬‡ï¸ MOVED DOWN: -5 per step
- ğŸ’€ FELL TO DEATH: -5
- ğŸš« OUT OF BOUNDS: -5

## Why This Fix Works

### 1. **Exploit is Impossible**
- Can't farm "landed" reward (one-time only)
- Can't farm "milestone" reward (requires persistence)
- Can't fake being "on step" (strict detection)

### 2. **Legitimate Play is Rewarded**
- Actually climbing gives +20 per step
- Staying on steps gives milestone bonuses
- Real progress is highly rewarded

### 3. **Agent Must Learn Real Skills**
- Must learn to jump ONTO steps
- Must learn to stay balanced
- Must learn to climb sequentially

## Training Recommendations

### Start Fresh
Since the old model learned the exploit:
```javascript
// Reset model and start over
app.modelManager.reset()
```

### Monitor Training
Watch for these signs of legitimate learning:
- âœ… Decreasing "landed" spam
- âœ… Increasing climbing attempts
- âœ… Agent trying to get ON steps
- âœ… Gradual height progress

### Expected Timeline
- **Episodes 0-500:** Learning to reach stairs
- **Episodes 500-2000:** Learning to jump onto steps
- **Episodes 2000-5000:** Learning to climb multiple steps
- **Episodes 5000+:** Consistent climbing behavior

## Related Concepts

### Reward Hacking
When an agent finds unintended ways to maximize reward without achieving the actual goal.

**Examples:**
- Touching steps instead of climbing (this case)
- Spinning in place to accumulate time rewards
- Exploiting physics glitches for position rewards

### Reward Shaping
Designing rewards to guide learning without creating exploits.

**Principles:**
- Reward the goal, not proxies
- Require proof of achievement
- Prevent reward farming
- Make exploitation harder than legitimate play

## Summary

**Problem:** Agent exploited lenient step detection to farm rewards without climbing

**Solution:**
- âœ… Stricter step detection (must be ABOVE step)
- âœ… Milestone rewards require persistence (3 frames)
- âœ… Landing reward only once per episode

**Result:**
- ğŸš« Exploit closed
- âœ… Agent must actually climb
- ğŸ“ˆ Genuine learning will emerge
- ğŸ¯ Rewards align with actual progress

The agent can no longer cheat - it must learn to climb for real! ğŸ§—â€â™‚ï¸
